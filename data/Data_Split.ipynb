{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-03-16T07:52:29.547687Z",
     "iopub.status.busy": "2023-03-16T07:52:29.546703Z",
     "iopub.status.idle": "2023-03-16T07:52:33.273282Z",
     "shell.execute_reply": "2023-03-16T07:52:33.272086Z",
     "shell.execute_reply.started": "2023-03-16T07:52:29.547624Z"
    },
    "id": "uXelPfgqgeIY",
    "outputId": "7e5948f9-a3b6-451a-fdfc-5971b951fb04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Collecting xmltodict\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4Rq-JjFjksT"
   },
   "source": [
    "# Natural Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T07:57:25.484922Z",
     "iopub.status.busy": "2023-03-16T07:57:25.484468Z",
     "iopub.status.idle": "2023-03-16T07:57:28.047031Z",
     "shell.execute_reply": "2023-03-16T07:57:28.046120Z",
     "shell.execute_reply.started": "2023-03-16T07:57:25.484894Z"
    },
    "id": "LQlKOpYFgb-y"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2afabd4d964266bd551d20f8967c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17746cfb1854459a663b867932e8b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536bc3e39d23486188c4af7bf1d8e5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import xmltodict\n",
    "from gzip import GzipFile\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuhTxTErXvyb"
   },
   "source": [
    "Galician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T10:00:24.479125Z",
     "iopub.status.busy": "2023-03-15T10:00:24.478790Z",
     "iopub.status.idle": "2023-03-15T10:00:25.193573Z",
     "shell.execute_reply": "2023-03-15T10:00:25.192826Z",
     "shell.execute_reply.started": "2023-03-15T10:00:24.479100Z"
    },
    "id": "aez-WseZhxte"
   },
   "outputs": [],
   "source": [
    "pairs_gl = []\n",
    "\n",
    "# import tmx to array\n",
    "def get_gl_pair(_, tree):\n",
    "    lang_pair = {}\n",
    "    for elem in tree['tuv']:\n",
    "        language = elem['@xml:lang']\n",
    "        text = elem['seg']\n",
    "        lang_pair[language] = text\n",
    "\n",
    "    pairs_gl.append(lang_pair)\n",
    "    return True\n",
    "\n",
    "xmltodict.parse(\n",
    "    GzipFile('natural_data/raw/en-gl.tmx.gz'),\n",
    "    item_depth=3, item_callback=get_gl_pair,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-03-15T10:00:27.336406Z",
     "iopub.status.busy": "2023-03-15T10:00:27.335583Z",
     "iopub.status.idle": "2023-03-15T10:00:45.582245Z",
     "shell.execute_reply": "2023-03-15T10:00:45.581677Z",
     "shell.execute_reply.started": "2023-03-15T10:00:27.336373Z"
    },
    "id": "RuhldSxZgGWb",
    "outputId": "dc0a1795-93ae-454c-b6e1-657b3b461813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence count:  33574\n",
      "Original data token count:  1132429\n",
      "Train sentence count:  26650\n",
      "Train token count:  900045\n",
      "Validation sentence count:  2957\n",
      "Validation token count:  100099\n",
      "Test sentence count:  2997\n",
      "Test token count:  100017\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the set\n",
    "pairs_gl = pd.DataFrame(pairs_gl).sample(frac=1)\n",
    "\n",
    "# Check original data\n",
    "tc_gl = 0\n",
    "for i in range(len(pairs_gl)):\n",
    "  if bool(pairs_gl.iloc[i]['gl']):\n",
    "    tc_gl += len(tokenizer(pairs_gl.iloc[i]['gl'])['input_ids'])\n",
    "print('Original sentence count: ', len(pairs_gl))\n",
    "print('Original data token count: ', tc_gl)\n",
    "\n",
    "# Splitting the data\n",
    "tc_train, tc_val, tc_test, index = 0, 0, 0, 0\n",
    "index_train, index_val, index_test = 0, 0, 0\n",
    "\n",
    "# Training set 900000 tokens\n",
    "while tc_train < 900000:\n",
    "  if bool(pairs_gl.iloc[index]['gl']):\n",
    "    tc_train += len(tokenizer(pairs_gl.iloc[index]['gl'])['input_ids'])\n",
    "  index += 1\n",
    "index_train = index\n",
    "print('Train sentence count: ', index)\n",
    "print('Train token count: ', tc_train)\n",
    "train_gl = pairs_gl[:index]\n",
    "pd.DataFrame(pd.DataFrame(train_gl)['gl']).to_csv('natural_data/train/gl-en.gl', header=False, index=False)\n",
    "pd.DataFrame(pd.DataFrame(train_gl)['en']).to_csv('natural_data/train/gl-en.en', header=False, index=False)\n",
    "\n",
    "# Validation set 100000 tokens\n",
    "while tc_val < 100000:\n",
    "  if bool(pairs_gl.iloc[index]['gl']):\n",
    "    tc_val += len(tokenizer(pairs_gl.iloc[index]['gl'])['input_ids'])\n",
    "  index += 1\n",
    "index_val = index\n",
    "print('Validation sentence count: ', index - index_train)\n",
    "print('Validation token count: ', tc_val)\n",
    "val_gl = pairs_gl[index_train:index]\n",
    "pd.DataFrame(pd.DataFrame(val_gl)['gl']).to_csv('natural_data/val/gl-en.gl', header=False, index=False)\n",
    "pd.DataFrame(pd.DataFrame(val_gl)['en']).to_csv('natural_data/val/gl-en.en', header=False, index=False)\n",
    "\n",
    "# Test set 100000 tokens\n",
    "while tc_test < 100000:\n",
    "  if bool(pairs_gl.iloc[index]['gl']):\n",
    "    tc_test += len(tokenizer(pairs_gl.iloc[index]['gl'])['input_ids'])\n",
    "  index += 1\n",
    "print('Test sentence count: ', index - index_val)\n",
    "print('Test token count: ', tc_test)\n",
    "test_gl = pairs_gl[index_val:index]\n",
    "pd.DataFrame(pd.DataFrame(val_gl)['gl']).to_csv('test_data/gl-en.gl', header=False, index=False)\n",
    "pd.DataFrame(pd.DataFrame(val_gl)['en']).to_csv('test_data/gl-en.en', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_PWmYfcVc6I"
   },
   "source": [
    "German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T07:24:59.641415Z",
     "iopub.status.busy": "2023-03-16T07:24:59.640156Z",
     "iopub.status.idle": "2023-03-16T07:25:12.903714Z",
     "shell.execute_reply": "2023-03-16T07:25:12.901376Z",
     "shell.execute_reply.started": "2023-03-16T07:24:59.641352Z"
    },
    "id": "q_5DsW65BgEo"
   },
   "outputs": [],
   "source": [
    "pairs_de = []\n",
    "\n",
    "# import tmx to array\n",
    "def get_de_pair(_, tree):\n",
    "    lang_pair = {}\n",
    "    for elem in tree['tuv']:\n",
    "        language = elem['@xml:lang']\n",
    "        text = elem['seg']\n",
    "        lang_pair[language] = text\n",
    "\n",
    "    pairs_de.append(lang_pair)\n",
    "    return True\n",
    "\n",
    "xmltodict.parse(\n",
    "    GzipFile('natural_data/raw/de-en.tmx.gz'),\n",
    "    item_depth=3, item_callback=get_de_pair,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-03-15T09:57:45.793204Z",
     "iopub.status.busy": "2023-03-15T09:57:45.792269Z",
     "iopub.status.idle": "2023-03-15T09:59:15.822296Z",
     "shell.execute_reply": "2023-03-15T09:59:15.821613Z",
     "shell.execute_reply.started": "2023-03-15T09:57:45.793174Z"
    },
    "id": "X6I_wkUkoIuk",
    "outputId": "3a7bfc35-967a-4420-b42f-d8f4cf0db807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence count:  289374\n",
      "Original data token count:  11438471\n",
      "Train sentence count:  22802\n",
      "Train token count:  900004\n",
      "Validation sentence count:  2489\n",
      "Validation token count:  100026\n",
      "Test sentence count:  2485\n",
      "Test token count:  100017\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the set\n",
    "pairs_de = pd.DataFrame(pairs_de).sample(frac=1)\n",
    "\n",
    "# Check original data\n",
    "tc_de = 0\n",
    "for i in range(len(pairs_de)):\n",
    "  if bool(pairs_de.iloc[i]['de']):\n",
    "    tc_de += len(tokenizer(pairs_de.iloc[i]['de'])['input_ids'])\n",
    "print('Original sentence count: ', len(pairs_de))\n",
    "print('Original data token count: ', tc_de)\n",
    "\n",
    "# Splitting the data\n",
    "tc_train, tc_val, tc_test, index = 0, 0, 0, 0\n",
    "index_train, index_val, index_test = 0, 0, 0\n",
    "\n",
    "# Training set 900000 tokens\n",
    "while tc_train < 900000:\n",
    "  if bool(pairs_de.iloc[index]['de']):\n",
    "    tc_train += len(tokenizer(pairs_de.iloc[index]['de'])['input_ids'])\n",
    "  index += 1\n",
    "index_train = index\n",
    "print('Train sentence count: ', index)\n",
    "print('Train token count: ', tc_train)\n",
    "train_de = pairs_de[:index]\n",
    "pd.DataFrame(pd.DataFrame(train_de)['de']).to_csv('natural_data/train/de-en.de', header=False, index=False)\n",
    "pd.DataFrame(pd.DataFrame(train_de)['en']).to_csv('natural_data/train/de-en.en', header=False, index=False)\n",
    "\n",
    "# Validation set 100000 tokens\n",
    "while tc_val < 100000:\n",
    "  if bool(pairs_de.iloc[index]['de']):\n",
    "    tc_val += len(tokenizer(pairs_de.iloc[index]['de'])['input_ids'])\n",
    "  index += 1\n",
    "index_val = index\n",
    "print('Validation sentence count: ', index - index_train)\n",
    "print('Validation token count: ', tc_val)\n",
    "val_de = pairs_de[index_train:index]\n",
    "pd.DataFrame(pd.DataFrame(val_de)['de']).to_csv('natural_data/val/de-en.de', header=False, index=False)\n",
    "pd.DataFrame(pd.DataFrame(val_de)['en']).to_csv('natural_data/val/de-en.en', header=False, index=False)\n",
    "\n",
    "# Test set 100000 tokens\n",
    "while tc_test < 100000:\n",
    "  if bool(pairs_de.iloc[index]['de']):\n",
    "    tc_test += len(tokenizer(pairs_de.iloc[index]['de'])['input_ids'])\n",
    "  index += 1\n",
    "print('Test sentence count: ', index - index_val)\n",
    "print('Test token count: ', tc_test)\n",
    "test_de = pairs_de[index_val:index]\n",
    "pd.DataFrame(pd.DataFrame(test_de)['de']).to_csv('test_data/de-en.de', header=False, index=False)\n",
    "pd.DataFrame(pd.DataFrame(test_de)['en']).to_csv('test_data/de-en.en', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T07:59:42.559608Z",
     "iopub.status.busy": "2023-03-16T07:59:42.559289Z",
     "iopub.status.idle": "2023-03-16T08:00:45.854251Z",
     "shell.execute_reply": "2023-03-16T08:00:45.853426Z",
     "shell.execute_reply.started": "2023-03-16T07:59:42.559583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence count:  133692\n",
      "Original data token count:  2609402\n",
      "Train sentence count:  46545\n",
      "Train token count:  900019\n",
      "Validation sentence count:  5138\n",
      "Validation token count:  100009\n"
     ]
    }
   ],
   "source": [
    "synth_gl = pd.read_csv('synthetic_data/raw/gl-en.gl', index_col=0)\n",
    "\n",
    "# Shuffle the set and remove duplicates\n",
    "synth_gl = pd.DataFrame(pd.DataFrame(synth_gl).sample(frac=1)['0'].unique())\n",
    "\n",
    "# Check original data\n",
    "tc_gl = 0\n",
    "for i in range(len(synth_gl)):\n",
    "  if bool(synth_gl.iloc[i][0]) and not pd.isna(synth_gl.iloc[i][0]):\n",
    "        tc_gl += len(tokenizer(synth_gl.iloc[i][0])['input_ids'])\n",
    "print('Original sentence count: ', len(synth_gl))\n",
    "print('Original data token count: ', tc_gl)\n",
    "\n",
    "# Splitting the data\n",
    "tc_train, tc_val, tc_test, index = 0, 0, 0, 0\n",
    "index_train, index_val, index_test = 0, 0, 0\n",
    "\n",
    "# Training set 900000 tokens\n",
    "while tc_train < 900000:\n",
    "  if bool(synth_gl.iloc[index][0]) and not pd.isna(synth_gl.iloc[index][0]):\n",
    "    tc_train += len(tokenizer(synth_gl.iloc[index][0])['input_ids'])\n",
    "  index += 1\n",
    "index_train = index\n",
    "print('Train sentence count: ', index)\n",
    "print('Train token count: ', tc_train)\n",
    "train_gl = synth_gl[:index]\n",
    "pd.DataFrame(pd.DataFrame(train_gl)).to_csv('synthetic_data/train/gl-en.gl', header=False, index=False)\n",
    "\n",
    "# Validation set 100000 tokens\n",
    "while tc_val < 100000:\n",
    "  if bool(synth_gl.iloc[index][0]) and not pd.isna(synth_gl.iloc[index][0]):\n",
    "    tc_val += len(tokenizer(synth_gl.iloc[index][0])['input_ids'])\n",
    "  index += 1\n",
    "index_val = index\n",
    "print('Validation sentence count: ', index - index_train)\n",
    "print('Validation token count: ', tc_val)\n",
    "val_gl = synth_gl[index_train:index]\n",
    "pd.DataFrame(pd.DataFrame(val_gl)).to_csv('synthetic_data/val/gl-en.gl', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T08:03:22.247228Z",
     "iopub.status.busy": "2023-03-16T08:03:22.246088Z",
     "iopub.status.idle": "2023-03-16T08:04:04.589396Z",
     "shell.execute_reply": "2023-03-16T08:04:04.588424Z",
     "shell.execute_reply.started": "2023-03-16T08:03:22.247200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence count:  71834\n",
      "Original data token count:  1166447\n",
      "Train sentence count:  55745\n",
      "Train token count:  900002\n",
      "Validation sentence count:  6112\n",
      "Validation token count:  100002\n"
     ]
    }
   ],
   "source": [
    "synth_de = pd.read_csv('synthetic_data/raw/de-en.de', header=None)\n",
    "\n",
    "# Shuffle the set and remove duplicates\n",
    "synth_de = pd.DataFrame(pd.DataFrame(synth_de).sample(frac=1)[0].unique())\n",
    "\n",
    "# Check original data\n",
    "tc_de = 0\n",
    "for i in range(len(synth_de)):\n",
    "  if bool(synth_de.iloc[i][0]) and not pd.isna(synth_de.iloc[i][0]):\n",
    "        tc_de += len(tokenizer(synth_de.iloc[i][0])['input_ids'])\n",
    "print('Original sentence count: ', len(synth_de))\n",
    "print('Original data token count: ', tc_de)\n",
    "\n",
    "# Splitting the data\n",
    "tc_train, tc_val, tc_test, index = 0, 0, 0, 0\n",
    "index_train, index_val, index_test = 0, 0, 0\n",
    "\n",
    "# Training set 900000 tokens\n",
    "while tc_train < 900000:\n",
    "  if bool(synth_de.iloc[index][0]) and not pd.isna(synth_de.iloc[index][0]):\n",
    "    tc_train += len(tokenizer(synth_de.iloc[index][0])['input_ids'])\n",
    "  index += 1\n",
    "index_train = index\n",
    "print('Train sentence count: ', index)\n",
    "print('Train token count: ', tc_train)\n",
    "train_de = synth_de[:index]\n",
    "pd.DataFrame(pd.DataFrame(train_de)).to_csv('synthetic_data/train/de-en.de', header=False, index=False)\n",
    "\n",
    "# Validation set 100000 tokens\n",
    "while tc_val < 100000:\n",
    "  if bool(synth_de.iloc[index][0]) and not pd.isna(synth_de.iloc[index][0]):\n",
    "    tc_val += len(tokenizer(synth_de.iloc[index][0])['input_ids'])\n",
    "  index += 1\n",
    "index_val = index\n",
    "print('Validation sentence count: ', index - index_train)\n",
    "print('Validation token count: ', tc_val)\n",
    "val_de = synth_de[index_train:index]\n",
    "pd.DataFrame(pd.DataFrame(val_de)).to_csv('synthetic_data/val/de-en.de', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
